---
title: "SVM Validation for all timepoints - AV Dataset"
output: html_notebook
---



```{r message=FALSE}

library(R.matlab)
library(sjmisc)
library(readxl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(kernelboot)
library(pROC)
library(effsize)
library(e1071)
library(pracma)
library(verification)
library(progress)
library(caret)
library(LiblineaR)
library(groupdata2)
library(data.table)
library(doParallel)
library(brms)

```

Defining with participant numbers belong to which group

```{r}

#Lists of participant numbers per groups, extracted from Excel file manually

CC_no = c('TVD_0001','TVD_0042','TVD_0277','TVD_0012','TVD_0016','TVD_0052','TVD_0070','TVD_0243','TVD_0229','TVD_0050','TVD_0256','TVD_0240','TVD_0145')

DC_no = c('TVD_0148','TVD_0144','TVD_0136','TVD_0127','TVD_0112','TVD_0104','TVD_0205','TVD_0206',
          'TVD_0010','TVD_0156','TVD_0059','TVD_0620','TVD_0622','TVD_0624','TVD_0627')

Control_no = c('CO_0001_','CO_0004_','CO_0005_','CO_0006_','CO_0009_','CO_0010_','CO_0011_','CO_0012_','CO_0013_','CO_0014_','CO_0015_','CO_0016_','CO_0017_','CO_0018_','CO_0022_','CO_0023_','CO_0024_','CO_0025_','CO_0026_','CO_0029_','CO_0030_','CO_0031_','CO_0032_','CO_0033_','CO_0036_','CO_0037_','CO_0038_','CO_0039_')

```


## Loading data for all timepoints

In order to analyze the participants data for all timepoints, the data for all participants must be rearranged into a list of dataframes, one for each timepoint. To this end, a list of 1000 empty dataframes is created first, with the dimensionality as needed (28x32). Then, a list of files that need to be read in is created. Finally, the dataframes for the single participants are read. Eac individual dataframe is of dimensions 32x1000, each row for one electrode and each column for one timepoint. By iterating over all columns, each column is written into one of the 1000 empty dataframes created earlier. The first column of is written in the first dataframe, the second in the second, and so on. Each column of a participants dataframe is written into a row in the single timepoints dataframes, whereby the columns represent the single electrodes. The first participants data therefore appears as the first row, the second participants data appears as the second row, and so on. We finally obtain a list of 1000 dataframes, one for each timepoint, with each row of each dataframe representing one participant and each column representing one electrode. 


```{r}


## Function to instantiate a list of 1000 dfs, 
instantiate_dflist <- function(nrows, ncols){
  df_list <- list() 
  for (i in seq(1000)){ 
    #create 1000 empty Matrices of desired dimensions
    df_list[[i]] <- data.frame(matrix(NA, nrow = nrows, ncol = ncols)) 
  }
  return(df_list)
}



## Function to store all filenames of a directory with a certain pattern in a list
get_filelist <- function(wd, pattern) {
  setwd(wd)
  filelist = list()
  #Extract all filenames from the given directory that match a given pattern
  for (file in list.files(wd, pattern=pattern, all.files=FALSE, full.names=FALSE)){
    #append each filename matching the pattern to the list
    filelist = append(filelist, file) 
  }
  return(filelist)
}

## Function to convert ms to index positions in df list
ms_index <- function(int){
  index <- int + 501
  return(index)
}

## Function to write participant data from .mat files in the list of empty dataframes.
write_files_to_list <- function(filelist, df_list, scale = TRUE){
  for (no_file in seq_along(filelist)) { #iterate over position in filelist
    df <- readMat(con = filelist[no_file]) # read .mat file from file position
    df <- data.frame(df$timelock[1]) # select 'avg' from matlab structure
    for (tp in seq_along(names(df))){ #iterate over timepoints (columns of dataframe)
      # The values of the electrodes are scaled over all electrodes for one timepoint(column)
      if (scale == TRUE){ 
        # Assign one column to one df of the df list with the row corresponding to file position.
        df_list[[tp]][no_file,1:32] <- scale(df[,tp]) 
      } else {
        df_list[[tp]][no_file,1:32] <- df[,tp]
      }
      # Participant number through first 8 characters of filename
      part_no <- substr(filelist[no_file],1,8) 
      df_list[[tp]][no_file,'participant_no'] <-  part_no  
      # Set group according to participant number
      if (part_no %in% CC_no){ 
      df_list[[tp]][no_file,'group'] <- "CC"
      } else if (part_no %in% DC_no){
        df_list[[tp]][no_file,'group'] <- "DC" 
      } else if (part_no %in% Control_no){
        df_list[[tp]][no_file,'group'] <- "Control"
      }  
    }
  }
  return(df_list)
}

```


##Cataract Dataframes

Application of the previously defined functions to the files within the cataract folder

```{r warning=FALSE}

df_list <- instantiate_dflist(28,32)

setwd("/home/bpn/Dokumente/Max/Data/Validation_Exp_2/cataract")

#get filenames from cataract directory
filelist_cat <- get_filelist(getwd(),"S20")

#Write control dataframes to list of dataframes
cat <- write_files_to_list(filelist_cat, df_list, scale = TRUE)

```


##Control dataframes

Application of the previously defined functions to the files within the control folder

```{r warning=FALSE}

setwd("/home/bpn/Dokumente/Max/Data/Validation_Exp_2/controls")

#get filenames from control directory
filelist_con <- get_filelist(getwd(),"S20")

#Write control dataframes to list of dataframes
con <- write_files_to_list(filelist_con, df_list, scale = TRUE)

```

##Combining both lists of dataframes

Creation of a new empty list of dataframes, this time with dimensions 58x34 (2 exta columns for participant number and group). Cataract individuals fill the first 26 rows and control individuals the last 26 rows for each timepoint.

The current total list of dataframes contains all electrodes (32). As the first analysis focuses only in the posterior electrodes, a new list of dataframes is created, containing only the electrodes (columns of single dfs) of interest. 

```{r}


total_dfs_val <- instantiate_dflist(56,34)

for (i in seq(1000)) {
  
  total_dfs_val[[i]][1:28,] <- cat[[i]]
  total_dfs_val[[i]][29:56,] <- con[[i]]
  names(total_dfs_val[[i]]) <- names(cat[[i]])
  
}


post_val <- total_dfs_val

for (i in seq(1000)) {
  
  post_val[[i]] <- post_val[[i]][,c(18:30,33,34)]
  
}

```

# ANALYSIS

# Defining functions for Analysis


```{r}

### Transform each dataframe according to needs
transform_df <- function(data, control = TRUE) {

  data <- data %>% mutate(isCC = factor(group != 'CC', labels = c("yes","no")))
  data$group <- factor(data$group, levels = c("CC", "DC", "Control"))

  if (control == FALSE) {
    
    data <- data[data$group %in% c('DC','CC'),]
    
  } else {
    
    data <- data
    
  }
  
  data <- data %>% dplyr::select(-c(participant_no, group))
  
  return(data)
  
}



### Combine SVM Weights to the final classifier
get_svm_classifier <- function(data, weights) {
  
  svmweights_replicate_rows <- do.call(rbind, replicate(dim(data)[1], weights, simplify=FALSE)) # replicate weights to match dimensions of dataframe
  DataToClassify_OnlyElecValues <- data %>% dplyr::select(-isCC) # Drop the isCC column
  classifier <- rowSums(DataToClassify_OnlyElecValues*svmweights_replicate_rows) # Compute classifier by multiplying features (electrodes) with their respective weights
  return(classifier)
  
}




## Function for manual CV with e1071
inner_CV <- function(df, k, hpt_grid){
  
  # Create empty dataframe to store CV results
  hpt_df_auc <- data.frame(matrix(NA, nrow = dim(hpt_grid)[1], ncol = k))
  names(hpt_df_auc) <- seq(k)
  
  # Create CV folds
  flds <- createFolds(df$isCC, k = k, list = TRUE, returnTrain = FALSE)
  
  # Loop over CV folds
  for (f in seq(k)){
    
    # Partitioning the Data in Training and Testing Data
    names(flds)[1:k] <- "train"
    names(flds)[f] <- "test"
    train_df <- df[-flds$test,]
    test_df <- df[flds$test,]
    
    # Loop over all Hyperparameter Grid Combinations    
    for (i in seq(dim(hpt_grid)[1])){
      
      # Training an SVM with the respective Hyperparameter Grid Combination
      hpt_svm <- svm(isCC ~ ., 
                     data = train_df, 
                     kernel = "linear", 
                     cost = hpt_grid[i,'cost'],
                     class.weights = c("no" = as.numeric(1/table(train_df$isCC)[2]), 
                                       "yes" = as.numeric(1/table(train_df$isCC)[1])),
                     scale = FALSE)
      
      # Deriving a Classifier from the previously fitted SVM and unseen test data
      clf <- get_svm_classifier(test_df, hpt_svm)

      # Compute AUC value for the classifier
      response <- ifelse(test_df$isCC == "yes",1,0)
      dir = ifelse(train_df$isCC[1] == "no", ">", "<") # direction depends on whether the first case is a CC or Non-CC
      rocObject <- roc(response, clf, direction = dir, print_auc = TRUE, quiet = TRUE)
      
      # Storing AUC values in dataframe
      hpt_df_auc[i,f] <- as.numeric(rocObject$auc)
      
    }
    
  }
  
  # Store mean AUCs for each Hyperparameter Combination over all folds with corresponding Hyperparameters
  cv_results <- data.frame(matrix(NA, 
                               nrow = dim(hpt_grid)[1], 
                               ncol = 2)) 
  
  # Determine Names of result set columns
  names(cv_results) <-c("cost", "AUC")
  
  # Assign values to result set columns
  cv_results[,"cost"] <- hpt_grid[,"cost"]
  cv_results[,"AUC"] <- rowMeans(hpt_df_auc)

  # Return Cost and Weight that yielded the best mean AUC over all folds. 
  # If more Combinations yielded the same best result, the first one is chosen
  best_tune <- cv_results[cv_results[,"AUC"] == max(cv_results$AUC), ]
  
  if (dim(best_tune)[1] > 1)
  output <- c(best_tune[1,]) else
  output <- c(best_tune)
  
  return(output)
  
}

# Function to obtain p-values of AUC analysis from Wilcoxon-Mann-Whitney-U Test 
auc_wmw <- function(response, scores){
  labels <- as.logical(response)
  pos <- scores[labels]
  neg <- scores[!labels]
  wilcox.test(pos, neg)$p.value
  
}




# Function to plot the extended validation curve: p-values from exp1 and Sensitivity + Specificity
plot_curve_val <- function(auc, lower_ci, upper_ci, p_exp2, thresholds, control,new = FALSE) {

  thresh_df = data.frame(t(as.data.frame(thresholds)), row.names = 1:601)
  
  viz <- data.frame(auc = auc, 
                  auc_low = lower_ci, 
                  auc_high = upper_ci,
                  p_exp2 = p_exp2,
                  sensitivity = thresh_df[,2],
                  specificity = thresh_df[,3])
  
  # Assign value of 0.35 to all significant p-values from Experiment 2, all other NA
  viz$p_exp2[viz$p_exp2 > 0.05] <- NA
  viz$p_exp2[viz$p_exp2 < 0.05] <- 0.35


  # Check which timepoints show a sensitivity + specificity of 1.5 or higher
  viz$sens_spec_sum <- viz$specificity + viz$sensitivity
  viz$sens_spec_sum[viz$sens_spec_sum < 1.5] <- NA
  viz$sens_spec_sum[viz$sens_spec_sum >= 1.5] <- 0.3

  # Set up timepoints
  viz["timepoint"] <- - 100 + seq(1,dim(viz)[1]*step,step)
  color_palette = c("#66c2a5", "#8da0cb")
  color = color_palette[ifelse(control,1,2)]
  
  {
  ggplot(data=viz, aes(x = timepoint)) + 
    geom_ribbon(aes(ymin = auc_low, ymax = auc_high), alpha = 0.3, fill = color) +
    geom_point(aes(y = p_exp2, col = color)) +
    geom_point(aes(y = sens_spec_sum, color = '#D55E00'), alpha = 0.5) +
    geom_line(aes(y = auc, color = color)) +
    scale_y_continuous(breaks = seq(0.2,1,0.2), 
                       labels = seq(0.2,1,0.2), 
                       limits = c(0,1),
                       expand = c(0,0)) +
    scale_x_continuous(expand = c(0,0)) +
    scale_colour_identity("AUC-Score", 
                      breaks = c(ifelse(!new, color, ""),"#D55E00"),
                      labels = c(ifelse(!new, "Significance", ""),"Sensitivity + Specificity ≥ 1.5"), # 
                      guide = "legend") +
    theme_bw(base_size = 15) +
    geom_hline(yintercept = 0.5, size = 0.5, linetype='dotted') +
    geom_vline(xintercept = 0, size = 0.2, linetype='dotted') +
    theme(axis.line = element_line(size = 0.2, color = 'black'),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          plot.background = element_blank(),
          panel.border = element_blank(),
          panel.grid.major.y = element_line(size=0.02, color="black"),
          legend.position=c(.55,.12),
          plot.title = element_text(size = 14,family="Helvetica", hjust = 0.5),
          axis.title.y = element_text(size = 10, angle = 90,family="Helvetica"),
          axis.title.x = element_text(size = 10, angle = 0, family="Helvetica"),
          axis.text.x = element_text(size = 8, colour="black"),
          axis.ticks.x = element_line(size = 0.2, colour="black"),
          axis.text.y = element_text(size = 8, colour="black"),
          axis.ticks.y = element_line(size = 0.2, colour="black"),
          legend.background = element_rect(fill="transparent", size=.2, linetype="dotted"),
          legend.text = element_text(colour="black", size = 10, family="Helvetica"),
          legend.title = element_blank(),
          legend.key.height = unit(0.1, 'cm'),
          legend.key.width = unit(0.5, 'cm')) +
    ggtitle(paste("AUC over timepoints - ",
                  ifelse(control, "CC vs. Non-CC", "CC vs. DC"),
                  ifelse(new, " (only new)",""),"
            ", sep = "")) +
    xlab("Timepoints in ms") +
    ylab("AUC") 
    
  }
}



```

## Running validation on all timepoints

```{r message=FALSE, warning=FALSE}

control = F

if (control)
  results_final = readRDS("Final_RDS_files/final_weights_all_part.RDS") else
  results_final = readRDS("Final_RDS_files/final_weights_CCDC.RDS") 
  

start <- Sys.time()

# Set parameters for nested CV 
milliseconds = 600 # The range of milliseconds we want to cover
step = 1    # step size with which to compute the classifications. 

# Instantiate empty vectors to store values
auc_list <- c()
auc_cilow_list <- c()
auc_cihi_list <- c()
threshold_val <- c()
test_ths <- c()
p_list <- c()
roc_objs <- c()

# Loop over all timepoints
for (ms in seq(0, milliseconds, step)){
  
  #Selecting the right dataframe
  offset <- 400
  df <- transform_df(post_val[[offset + ms]], control = control) 
  weights <- as.vector(results_final[[ms+1]]$weights)
  
  ## Which format of weights do we need for validation?  
  weights_as_rows <- do.call(rbind, 
                             replicate(dim(df)[1],
                                       weights,
                                       simplify=FALSE)) 
  clf_val <- rowSums(df[,-ncol(df)]*weights_as_rows)
      
  # Remap the response to binary for pROC to understand 
  response <- ifelse(df$isCC == "yes",1,0)
  
  # Fit ROC with correctly formatted response and direction
  rocObject <- roc(response, 
                   clf_val, 
                   direction = "<",
                   print_auc = TRUE, 
                   quiet = TRUE)
  
  threshold_validation =  coords(rocObject, results_final[[ms+1]]$threshold, input = "threshold", transpose = TRUE)
  test_th  =  coords(rocObject, x = "best", method = "y", transpose = TRUE)
  conf_int = ci.auc(rocObject)
  
  # Write single repetition- and fold-results into list for later averaging
  auc_list <- c(auc_list,rocObject$auc) # AUC value
  auc_cilow_list <- c(auc_cilow_list,conf_int[1])
  auc_cihi_list <- c(auc_cihi_list,conf_int[3])
  threshold_val <- c(threshold_val, list(threshold_validation))
  test_ths <- c(test_ths, test_th)
  
  d = data.frame("clf" = clf_val, "response" = response)
  p_list <- c(p_list,auc_wmw(response,clf_val)) # p-value from WMW Test
  roc_objs <- c(roc_objs, rocObject)
  
}

p_exp2 <- p.adjust(p_list, method = "BH")

# Load timepoint significance from experiment 1
plot_curve_val(auc_list,auc_cilow_list ,auc_cihi_list, p_exp2, threshold_val, control)

## Save significant timepoints
sig_tp <- which(p_exp2 < 0.05)
#saveRDS(sig_tp, ifelse(control,"Final_RDS_resultssig_all.RDS","sig_dc.RDS"))
        
```

# Helper Code to extract which timepoints were significant and which timepoints exhibited a sum of Sensitivity and Specificity above 1.5


```{r}

`%notin%` <- Negate(`%in%`)

cat("AUCs  \n") # Print what's being displayed

# Aim: Get Intervals with consecutive significant timepoints.
#Iterate over significant timepoints; if they are preceded of followed by a significant timepoint, nothin happens. 
#If a timepoint before or after a significant timepoint is not significant, then the timepoint is either the start or end point of an interval or a #single significant timepoint. Single significant timepoints further have no significant timepoint before AND after.

sig_tp <- which(p_exp2 < 0.05) # Extract significant timepoints
interval_bounds = c()
single_tps = c()
for (i in seq_along(sig_tp)) { 
  tp = sig_tp[i]
  if (((tp-1) %notin% sig_tp) | ((tp+1) %notin% sig_tp)) interval_bounds = c(interval_bounds, tp) # Determines the start or the end of an interval
  if (((tp-1) %notin% sig_tp) & ((tp+1) %notin% sig_tp)) single_tps = c(single_tps, tp) # 
}

intervals <- interval_bounds[which(interval_bounds %notin% single_tps)] # Check that intervals and single tps are not intermixed

# Extract maximum AUC from each interval (or cluster) and the corresponding peak timepoint and print it
for (i in seq(1, length(intervals),2)){
  cluster = auc_list[intervals[i]:intervals[i+1]]
  max_clust = which(cluster == max(cluster))
  peak = intervals[i] + max_clust - 1
  print(paste(intervals[i] - 101,"to",intervals[i+1] - 101, ":","AUC range from",round(min(cluster),3),"to", round(max(cluster),3)))
}

# Print single Timepoints and corresponsing AUC value
for (i in seq(1, length(single_tps))){
  tp = single_tps[i]
  auc = auc_list[tp] 
  print(paste("Timepoint", tp-101, ":","AUC of",round(auc,3)))
}


#### Sensitivity & Specificity

# Same logic applies as for the significant timepoints, only the objective here is different: minimum 1.5 sum of sens + spec, again intervals and single timepoints are extracted in the same manner as above
cat("\nSensitivity & Specificity  \n")
thresh_df = data.frame(t(as.data.frame(threshold_val)), row.names = 1:601)
sensspec <- data.frame(sensitivity = thresh_df[,3],
                specificity = thresh_df[,2])
sensspec$sens_spec_sum <- sensspec$specificity + sensspec$sensitivity
sensspec_tp <- which(sensspec$sens_spec_sum >= 1.5)

# Find interval bounds of high sens + spec intervals and single timepoints with high sens + spec
interval_bounds_ss = c()
single_tps_ss = c()
for (i in seq_along(sensspec_tp)) {
  tp = sensspec_tp[i]
  if (((tp-1) %notin% sensspec_tp) | ((tp+1) %notin% sensspec_tp)) interval_bounds_ss = c(interval_bounds_ss, tp)
  if (((tp-1) %notin% sensspec_tp) & ((tp+1) %notin% sensspec_tp)) single_tps_ss = c(single_tps_ss, tp)
}

intervals_ss <- interval_bounds_ss[which(interval_bounds_ss %notin% single_tps_ss)] # Separate intervals from single timepoints

sensspec_list <- c(sensspec$sens_spec_sum)
sens_list <- c(sensspec$sensitivity)
spec_list <- c(sensspec$specificity)

# Extract maximum Sum of Sens + Spec from each interval (or cluster) and the corresponding peak timepoint and print it
for (i in seq(1, length(intervals_ss),2)){
  cluster = sensspec_list[intervals_ss[i]:intervals_ss[i+1]]
  max_clust = which(cluster == max(cluster))[1] 
  peak = intervals_ss[i] + max_clust - 1
  print(paste(intervals_ss[i] - 101,"to",intervals_ss[i+1] - 101, ":"," Sensitivity of",round(sens_list[peak],3),"and Specificity of", round(spec_list[peak],3)))
}

# Extract sum of Sens + Spec of single timepoints
for (i in seq(1, length(single_tps_ss))){
  tp = single_tps_ss[i]
  auc = auc_list[tp] 
  print(paste("Timepoint", tp-101,":","Sensitivity of",round(sens_list[tp],3),"and Specificity of", round(spec_list[tp],3)))
}


```
 
# Validation for individuals unique to Experiment 2

```{r}

# Participant IDs per experiment

## Experiment 1
CC_1 = c("TVD_0001", "TVD_0012", "TVD_0015", "TVD_0016", "TVD_0042", "TVD_0052", "TVD_0070", "TVD_0243", "TVD_0274", "TVD_0240", "TVD_0256", "TVD_0050")

DC_1 = c("TVD_0200", "TVD_0206", "TVD_0144", "TVD_0177", "TVD_0136", "TVD_0181", "TVD_0127", "TVD_0156", "TVD_0205", "TVD_0010", "TVD_0148", "TVD_0135", "TVD_0606")

## Experiment 2
CC_2 = c('TVD_0001','TVD_0042','TVD_0277','TVD_0012','TVD_0016','TVD_0052','TVD_0070','TVD_0243','TVD_0229','TVD_0050','TVD_0256','TVD_0240','TVD_0145')

DC_2 = c('TVD_0148','TVD_0144','TVD_0136','TVD_0127','TVD_0112','TVD_0104','TVD_0205','TVD_0010','TVD_0156','TVD_0059','TVD_0620','TVD_0622','TVD_0624','TVD_0627','TVD_0206')

# Build function to subset IDs
`%notin%` <- Negate(`%in%`)

# Select participants that are not included in experiment 1
CC_new = c()
for (i in CC_2) if (i %notin% CC_1) CC_new = append(CC_new, i)

DC_new = c()
for (i in DC_2) if (i %notin% DC_1) DC_new = append(DC_new, i)

# Select all Controls for comparability to Suddha's study
for (i in Control_no) Control_new = append(Control_new, i)

# Combine IDs of all participants new to experiment 2
all_new = c(CC_new, DC_new, Control_new)



control = F

if (control)
  results_final = readRDS("Final_RDS_files/final_weights_all_part.RDS") else
  results_final = readRDS("Final_RDS_files/final_weights_CCDC.RDS") 


start <- Sys.time()

# Set parameters for nested CV 
milliseconds = 600 # The range of milliseconds we want to cover
step = 1    # step size with which to compute the classifications. 

# Instantiate empty vectors to store values
auc_list <- c()
auc_cilow_list <- c()
auc_cihi_list <- c()
threshold_val <- c()
test_ths <- c()
p_list <- c()
roc_objs <- c()

# Loop over all timepoints
for (ms in seq(0, milliseconds, step)){
  
  #Selecting the right dataframe
  offset <- 400
  df <- transform_df(post_val[[offset + ms]][post_val[[offset + ms]]$participant_no %in% all_new,], control = control)
  weights <- as.vector(results_final[[ms+1]]$weights)
  
  ## Which format of weights do we need for validation?  
  weights_as_rows <- do.call(rbind, 
                             replicate(dim(df)[1],
                                       weights,
                                       simplify=FALSE)) 
  clf_val <- rowSums(df[,-ncol(df)]*weights_as_rows)
      
  # Remap the response to binary for pROC to understand 
  response <- ifelse(df$isCC == "yes",1,0)
  
  # Fit ROC with correctly formatted response and direction
  rocObject <- roc(response, 
                   clf_val, 
                   direction = "<",
                   print_auc = TRUE, 
                   quiet = TRUE)
  
  threshold_validation =  coords(rocObject, results_final[[ms+1]]$threshold, input = "threshold", transpose = TRUE)
  test_th  =  coords(rocObject, x = "best", method = "y", transpose = TRUE)
  conf_int = ci.auc(rocObject)
  
  # Write single repetition- and fold-results into list for later averaging
  auc_list <- c(auc_list,rocObject$auc) # AUC value
  auc_cilow_list <- c(auc_cilow_list,conf_int[1])
  auc_cihi_list <- c(auc_cihi_list,conf_int[3])
  threshold_val <- c(threshold_val, list(threshold_validation))
  test_ths <- c(test_ths, test_th)
  
  d = data.frame("clf" = clf_val, "response" = response)
  p_list <- c(p_list,auc_wmw(response,clf_val)) # p-value from WMW Test
  roc_objs <- c(roc_objs, rocObject)
  
}
# Benjamini Hochberg Correction of p-values
p_adj <- p.adjust(p_list, method = "BH")

stop <- Sys.time()
stop - start

# Plot Validation curve
plot_curve_val(auc_list,auc_cilow_list ,auc_cihi_list, p_adj, threshold_val, control, new = TRUE)

```


# Helper Code to extract which timepoints were significant and which timepoints exhibited a sum of Sensitivity and Specificity above 1.5

```{r}

`%notin%` <- Negate(`%in%`)

cat("AUCs  \n") # Print what's being displayed

# Aim: Get Intervals with consecutive significant timepoints.
#Iterate over significant timepoints; if they are preceded of followed by a significant timepoint, nothin happens. 
#If a timepoint before or after a significant timepoint is not significant, then the timepoint is either the start or end point of an interval or a #single significant timepoint. Single significant timepoints further have no significant timepoint before AND after.

sig_tp <- which(p_exp2 < 0.05) # Extract significant timepoints
interval_bounds = c()
single_tps = c()
for (i in seq_along(sig_tp)) { 
  tp = sig_tp[i]
  if (((tp-1) %notin% sig_tp) | ((tp+1) %notin% sig_tp)) interval_bounds = c(interval_bounds, tp) # Determines the start or the end of an interval
  if (((tp-1) %notin% sig_tp) & ((tp+1) %notin% sig_tp)) single_tps = c(single_tps, tp) # 
}

intervals <- interval_bounds[which(interval_bounds %notin% single_tps)] # Check that intervals and single tps are not intermixed

# Extract maximum AUC from each interval (or cluster) and the corresponding peak timepoint and print it
for (i in seq(1, length(intervals),2)){
  cluster = auc_list[intervals[i]:intervals[i+1]]
  max_clust = which(cluster == max(cluster))
  peak = intervals[i] + max_clust - 1
  print(paste(intervals[i] - 101,"to",intervals[i+1] - 101, ":","AUC range from",round(min(cluster),3),"to", round(max(cluster),3)))
}

# Print single Timepoints and corresponsing AUC value
for (i in seq(1, length(single_tps))){
  tp = single_tps[i]
  auc = auc_list[tp] 
  print(paste("Timepoint", tp-101, ":","AUC of",round(auc,3)))
}


#### Sensitivity & Specificity

# Same logic applies as for the significant timepoints, only the objective here is different: minimum 1.5 sum of sens + spec, again intervals and single timepoints are extracted in the same manner as above
cat("\nSensitivity & Specificity  \n")
thresh_df = data.frame(t(as.data.frame(threshold_val)), row.names = 1:601)
sensspec <- data.frame(sensitivity = thresh_df[,3],
                specificity = thresh_df[,2])
sensspec$sens_spec_sum <- sensspec$specificity + sensspec$sensitivity
sensspec_tp <- which(sensspec$sens_spec_sum >= 1.5)

# Find interval bounds of high sens + spec intervals and single timepoints with high sens + spec
interval_bounds_ss = c()
single_tps_ss = c()
for (i in seq_along(sensspec_tp)) {
  tp = sensspec_tp[i]
  if (((tp-1) %notin% sensspec_tp) | ((tp+1) %notin% sensspec_tp)) interval_bounds_ss = c(interval_bounds_ss, tp)
  if (((tp-1) %notin% sensspec_tp) & ((tp+1) %notin% sensspec_tp)) single_tps_ss = c(single_tps_ss, tp)
}

intervals_ss <- interval_bounds_ss[which(interval_bounds_ss %notin% single_tps_ss)] # Separate intervals from single timepoints

sensspec_list <- c(sensspec$sens_spec_sum)
sens_list <- c(sensspec$sensitivity)
spec_list <- c(sensspec$specificity)

# Extract maximum Sum of Sens + Spec from each interval (or cluster) and the corresponding peak timepoint and print it
for (i in seq(1, length(intervals_ss),2)){
  cluster = sensspec_list[intervals_ss[i]:intervals_ss[i+1]]
  max_clust = which(cluster == max(cluster))[1] 
  peak = intervals_ss[i] + max_clust - 1
  print(paste(intervals_ss[i] - 101,"to",intervals_ss[i+1] - 101, ":"," Sensitivity of",round(sens_list[peak],3),"and Specificity of", round(spec_list[peak],3)))
}

# Extract sum of Sens + Spec of single timepoints
for (i in seq(1, length(single_tps_ss))){
  tp = single_tps_ss[i]
  auc = auc_list[tp] 
  print(paste("Timepoint", tp-101,":","Sensitivity of",round(sens_list[tp],3),"and Specificity of", round(spec_list[tp],3)))
}


```


